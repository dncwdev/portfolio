# Portfolio — AI Infrastructure Engineer

AI Infrastructure Engineer specializing in GPU-based LLM platforms, air-gapped/on-prem architectures, and distributed inference systems.

- Built and operated an internal LLM platform in a fully isolated enterprise environment (Kubernetes + NVIDIA GPUs + vLLM/Ollama + Open-WebUI).
- Operated multi-node NVIDIA DGX systems supporting distributed inference and large-scale experimentation.
- Implemented RAG pipelines, monitoring stacks (Grafana), and access-controlled inference endpoints.
- Managed on-prem data center operations: GPU servers, storage/network design, Oracle DB, and WebLogic services.
- Led cross-functional initiatives bridging AI engineering, infrastructure, and business requirements.

> This repository contains sanitized case studies and minimal reproductions. Proprietary details are removed.

## Projects

| Project | Summary | Tech | Link |
| --- | --- | --- | --- |
| Container Platform (Case Study) | Docker/Compose runtime + ops runbook + minimal reproduction | Docker, Compose, Prometheus, Grafana | [README](projects/container-platform/README.md) |

## Skills

- **LLM / Inference:** `vLLM` · `Triton` · `Ollama` · `RAG`
- **Platform / Infra:** `Kubernetes` · `NVIDIA DGX` · `Linux` · `Air-gapped / On-prem`
- **Observability / Ops:** `Grafana` · `Prometheus` · `Docker/Compose` · `DevOps`
- **Enterprise:** `Oracle DB` · `WebLogic` · `Access control` · `Regulated-data compliance`
- **Certifications:** `CKA` · `PMP`

## Experience

- Public-sector energy enterprise (South Korea) — Section Manager / AI Infrastructure Engineer (May 2005–Present)
  - Air-gapped enterprise LLM platform (Kubernetes, vLLM, Ollama, NVIDIA GPUs)
  - Multi-node DGX operations for distributed inference
  - RAG pipelines + Grafana monitoring + secure model-serving environment
  - On-prem data center ops: GPU servers, storage/network, Oracle DB, WebLogic
  - Cross-functional AI infrastructure initiatives

## Contact

- LinkedIn: <add link>
